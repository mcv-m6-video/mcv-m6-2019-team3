{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Fine-tune to your data YOLOv2\n",
    "\n",
    "This notebook illustrates the method followed in order to perform the fine-tune one of the most famous object Detectors in Deep Learning: YOLO. \n",
    "We decided to fine-tune YOLO, because it’s usually faster than the others, and it allows us to have more time to deal with developing inconveniences. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv2\n",
    "\n",
    "We tried to adapt the provided code from github\n",
    "\n",
    "https://github.com/experiencor/keras-yolo2\n",
    "\n",
    "following these steps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Data preparation:\n",
    "Organizing the dataset into 4 folders \n",
    "\n",
    "train_image_folder <= the folder that contains the train images.\n",
    "\n",
    "train_annot_folder <= the folder that contains the train annotations in VOC format.\n",
    "\n",
    "valid_image_folder <= the folder that contains the validation images.\n",
    "\n",
    "valid_annot_folder <= the folder that contains the validation annotations in VOC format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to convert our annotations in VOC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "from PIL import Image\n",
    "from reading import read_annotations_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_xml_path = \"C:\\\\Users\\\\sarac\\\\Desktop\\\\CLASE\\\\M6\\\\yolo_custom\\\\keras-yolo2\\\\m6-full_annotation.xml\"\n",
    "video_path = \"C:\\\\Users\\\\sarac\\\\Desktop\\\\CLASE\\\\M6\\\\yolo_custom\\\\keras-yolo2\\\\vdo.avi\"\n",
    "trainpath = \"C:\\\\Users\\\\sarac\\\\Desktop\\\\CLASE\\\\M6\\\\yolo_custom\\\\keras-yolo2\\\\cars\\\\images\\\\val\\\\*.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,file in enumerate(glob.glob(trainpath)):\n",
    "    print(n)\n",
    "    annotation = ET.Element('annotation')\n",
    "    annotation.text = '\\n'    \n",
    "    folder = ET.SubElement(annotation, 'folder')\n",
    "    folder.text = 'images'\n",
    "    folder.tail = '\\n'  # empty line after the celldata element\n",
    "    filename = ET.SubElement(annotation, 'filename')\n",
    "    filename.text = '{}'.format(os.path.basename(file))\n",
    "    filename.tail = '\\n'\n",
    "    path = ET.SubElement(annotation, 'path')\n",
    "    path.text = '{}'.format(file)\n",
    "    path.tail = '\\n'\n",
    "    source = ET.SubElement(annotation, 'source')\n",
    "    source.text = '\\n'\n",
    "    source.tail = '\\n'\n",
    "    database = ET.SubElement(source, 'database')\n",
    "    database.text = 'Unknown'\n",
    "    database.tail = '\\n'\n",
    "\n",
    "    print(file)\n",
    "    im = Image.open(file)\n",
    "    w, h = im.size\n",
    "    size = ET.SubElement(annotation, 'size')\n",
    "    size.text = '\\n'\n",
    "    size.tail = '\\n'\n",
    "    width = ET.SubElement(size, 'width')\n",
    "    width.text = '{}'.format(w)\n",
    "    width.tail = '\\n'\n",
    "    height = ET.SubElement(size, 'height')\n",
    "    height.text = '{}'.format(h)\n",
    "    height.tail = '\\n'\n",
    "    depth = ET.SubElement(size, 'depth')\n",
    "    depth.text = '3'\n",
    "    depth.tail = '\\n'\n",
    "    segmented = ET.SubElement(annotation, 'segmented')\n",
    "    segmented.text = '0'\n",
    "    segmented.tail = '\\n'\n",
    "\n",
    "    objs = [x for x in groundtruth_list if x.frame==n+535]\n",
    "    for obj in objs:\n",
    "        object = ET.SubElement(annotation, 'object')\n",
    "        object.text = '\\n'\n",
    "        object.tail = '\\n'\n",
    "        name = ET.SubElement(object, 'name')\n",
    "        name.text = 'car'\n",
    "        name.tail = '\\n'\n",
    "        pose = ET.SubElement(object, 'pose')\n",
    "        pose.text = 'Unspecified'\n",
    "        pose.tail = '\\n'\n",
    "        truncated = ET.SubElement(object, 'truncated')\n",
    "        truncated.text = '0'\n",
    "        truncated.tail = '\\n'\n",
    "        difficult = ET.SubElement(object, 'difficult')\n",
    "        difficult.text = '0'\n",
    "        difficult.tail = '\\n'\n",
    "        bndbox = ET.SubElement(object, 'bndbox')\n",
    "        bndbox.text = '\\n'\n",
    "        bndbox.tail = '\\n'\n",
    "        xmin = ET.SubElement(bndbox, 'xmin')\n",
    "        xmin.text = '{}'.format(obj.xtl)\n",
    "        xmin.tail = '\\n'\n",
    "        ymin = ET.SubElement(bndbox, 'ymin')\n",
    "        ymin.text = '{}'.format(obj.ytl)\n",
    "        ymin.tail = '\\n'\n",
    "        xmax = ET.SubElement(bndbox, 'xmax')\n",
    "        xmax.text = '{}'.format(obj.width)\n",
    "        xmax.tail = '\\n'\n",
    "        ymax = ET.SubElement(bndbox, 'ymax')\n",
    "        ymax.text = '{}'.format(obj.height)\n",
    "        ymax.tail = '\\n'\n",
    "        \n",
    "    tree = ET.ElementTree(annotation)\n",
    "    tree.write(\"xmllabels\\\\{}.xml\".format(os.path.splitext(os.path.basename(file))[0],\n",
    "               encoding='utf-8', xml_declaration=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-9bb47ea802b8>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-9bb47ea802b8>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    <annotation>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Example of VOC format annotations.\n",
    "\n",
    "<annotation>\n",
    "    <folder>images</folder>\n",
    "    <filename>frame_0001.jpg</filename>\n",
    "    <path>C:\\Users\\sarac\\Desktop\\CLASE\\M6\\yolo_custom\\keras-yolo2\\cars\\images\\train\\frame_0001.jpg</path>\n",
    "    <source>\n",
    "        <database>Unknown</database>\n",
    "    </source>\n",
    "    <size>\n",
    "        <width>1920</width>\n",
    "        <height>1080</height>\n",
    "        <depth>3</depth>\n",
    "    </size>\n",
    "    <segmented>0</segmented>\n",
    "    <object>\n",
    "        <name>car</name>\n",
    "        <pose>Unspecified</pose>\n",
    "        <truncated>0</truncated>\n",
    "        <difficult>0</difficult>\n",
    "        <bndbox>\n",
    "            <xmin>558</xmin>\n",
    "            <ymin>94</ymin>\n",
    "            <xmax>663</xmax>\n",
    "            <ymax>169</ymax>\n",
    "        </bndbox>\n",
    "    </object>\n",
    "    <object>\n",
    "        <name>car</name>\n",
    "        <pose>Unspecified</pose>\n",
    "        <truncated>0</truncated>\n",
    "        <difficult>0</difficult>\n",
    "        <bndbox>\n",
    "            <xmin>1285</xmin>\n",
    "            <ymin>363</ymin>\n",
    "            <xmax>1516</xmax>\n",
    "            <ymax>546</ymax>\n",
    "        </bndbox>\n",
    "    </object>\n",
    "    <object>\n",
    "        <name>car</name>\n",
    "        <pose>Unspecified</pose>\n",
    "        <truncated>0</truncated>\n",
    "        <difficult>0</difficult>\n",
    "        <bndbox>\n",
    "            <xmin>931</xmin>\n",
    "            <ymin>78</ymin>\n",
    "            <xmax>1013</xmax>\n",
    "            <ymax>146</ymax>\n",
    "        </bndbox>\n",
    "    </object>\n",
    "</annotation>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Edit the configuration file\n",
    "Tiny Yolo using weights from mask-RCNN trained with COCO, change input size, paths to folders..\n",
    "\n",
    "\n",
    "{\n",
    "    \"model\" : {\n",
    "        \"backend\":              \"Tiny Yolo\",\n",
    "        \"input_size\":           480,\n",
    "        \"anchors\":              [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828],\n",
    "        \"max_box_per_image\":    10,\n",
    "        \"labels\":               [\"car\"]\n",
    "    },\n",
    "\n",
    "    \"train\": {\n",
    "        \"train_image_folder\":   \"/home/grupo04/m6/yolo_custom/keras-yolo2/dataset/train_image_folder/\",\n",
    "        \"train_annot_folder\":   \"/home/grupo04/m6/yolo_custom/keras-yolo2/dataset/train_annot_folder/\",\n",
    "\n",
    "        \"train_times\":          5,\n",
    "        \"pretrained_weights\":   \"\",\n",
    "        \"batch_size\":           8,\n",
    "        \"learning_rate\":        1e-4,\n",
    "        \"nb_epochs\":            1,\n",
    "        \"warmup_epochs\":        2,\n",
    "\n",
    "        \"object_scale\":         3.0 ,\n",
    "        \"no_object_scale\":      1.0,\n",
    "\n",
    "        \"coord_scale\":          1.0,\n",
    "        \"class_scale\":          1.0,\n",
    "\n",
    "        \"saved_weights_name\":   \"yolo-tiny-small-in.h5\",\n",
    "        \"debug\":                true\n",
    "    },\n",
    "\n",
    "    \"valid\": {\n",
    "        \"valid_image_folder\":    \"/home/grupo04/m6/yolo_custom/keras-yolo2/dataset/valid_image_folder/\",\n",
    "        \"valid_annot_folder\":    \"/home/grupo04/m6/yolo_custom/keras-yolo2/dataset/valid_annot_folder/\",\n",
    "\n",
    "        \"valid_times\":          1\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate anchors for our dataset: \n",
    "Run the following and copy the generated anchors printed on the terminal to the anchors setting in configuration file\n",
    "Start the training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-189172816f16>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-189172816f16>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python gen_anchors.py -c config.json\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python gen_anchors.py -c config.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Start the training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py -c config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problems\n",
    "We found some problems training this network, because input size in this case is so big (1920x1080). We didn’t find a way to reduce the input size without compromising the framework development. Therefore the resulting network was 1) too heavy and we’d been force to have a batch size of 8 (if not, we exceeded the memory limit of the server). Big sizes and slow batch size, result in a 2) very slow training. We have to reduce the number of epochs until 3 and it last more than 6 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Results\n",
    "#### Problems\n",
    "We found some problems training this network, because input size in this case is so big (1920x1080). We didn’t find a way to reduce the input size without compromising the framework development. Therefore the resulting network was 1) too heavy and we’d been force to have a batch size of 8 (if not, we exceeded the memory limit of the server). Big sizes and slow batch size, result in a 2) very slow training. We have to reduce the number of epochs until 3 and it last more than 6 hours.\n",
    "\n",
    "#### Results - Improvements\n",
    "With these premises, we can not achieve satisfactory results and for the next we will try to implement our own code instead of using external frameworks\n",
    "\n",
    "##### Epoch 00000: val_loss improved from inf to 10.00356, saving model to yolo-tiny-small-in.h5\n",
    "4308s - loss: 10.0414 - val_loss: 10.0036\n",
    "\n",
    "##### Epoch 00001: val_loss improved from 10.00356 to 10.00269, saving model to yolo-tiny-small-in.h5\n",
    "3522s - loss: 10.0029 - val_loss: 10.0027\n",
    "\n",
    "##### Epoch 00002: val_loss improved from 10.00269 to 0.05315, saving model to yolo-tiny-small-in.h5\n",
    "4241s - loss: 0.0486 - val_loss: 0.0531\n",
    "\n",
    "##### Final mAP\n",
    "mAP: 0.3030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
